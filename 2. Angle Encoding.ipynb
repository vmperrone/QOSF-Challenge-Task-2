{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "732321ef",
   "metadata": {},
   "source": [
    "The data is loaded in and preprocessed almost the same as last time. The only difference is the first column does not need to be divided by 1000 as we are going to normalize the data column-wise instead of row-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "011a0b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pennylane import numpy as np\n",
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=4)\n",
    "\n",
    "train_set = np.loadtxt(open(\"mock_train_set.csv\"), delimiter=\",\")\n",
    "test_set = np.loadtxt(open(\"mock_test_set.csv\"), delimiter=\",\")\n",
    "\n",
    "def preprocessing(data):\n",
    "    data[1:,1] = np.log10(data[1:,1])\n",
    "    data[1:,2] = np.log10(data[1:,2])\n",
    "    data[1:,3] = np.sin(data[1:,3]*np.pi/180)\n",
    "    data[1:,4] = 2 * data[1:,4] - 1\n",
    "    return data\n",
    "\n",
    "train_set = preprocessing(train_set)\n",
    "test_set = preprocessing(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93800ba",
   "metadata": {},
   "source": [
    "Next we normalize the data to be in the interval [-π,π]. Unlike the amplitude encoding, we normalize the data column-wise instead of row-wise, as we are not normalizing the statevector, but rather ensuring our data falls within an interval where the rotations are uniquely defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8ca12277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 rows of raw train_set data:\n",
      " [[2789.26       3.         1.         0.34202   -1.     ]\n",
      " [4040.01       6.         0.         0.01745    1.     ]\n",
      " [2931.2        4.         4.         0.64279    1.     ]]\n",
      "First 3 rows of normalized train_set data:\n",
      " [[ 1.75408  1.5708   0.5236   0.34202 -1.     ]\n",
      " [ 2.54064  3.14159  0.       0.01745  1.     ]\n",
      " [ 1.84335  2.0944   2.0944   0.64279  1.     ]]\n"
     ]
    }
   ],
   "source": [
    "# Normalizes data between -π and π\n",
    "def get_norms(x):\n",
    "    n_rows, n_cols = x.shape\n",
    "    maxes = np.max(np.abs(x), axis=0)\n",
    "    params = x / np.tile(maxes, (n_rows, 1))\n",
    "    params[:,0:3] *= np.pi\n",
    "    return params\n",
    "\n",
    "print(\"First 3 rows of raw train_set data:\\n\", train_set[1:,:][0:3])\n",
    "\n",
    "train_set = get_norms(train_set[1:])\n",
    "test_set = get_norms(test_set[1:])\n",
    "\n",
    "print(\"First 3 rows of normalized train_set data:\\n\", train_set[:][0:3])\n",
    "\n",
    "features = train_set[:,0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1954c5",
   "metadata": {},
   "source": [
    "Our circuit for the angle encoding is considerably simpler than for the amplitude encoding. It entails just a single rotation gate on each qubit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c313852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares state with rotation gates\n",
    "def state_preparation(a):\n",
    "    qml.RY(a[0], wires=0)\n",
    "    qml.RY(a[1], wires=1)\n",
    "    qml.RY(a[2], wires=2)\n",
    "    qml.RY(a[3], wires=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642d4b68",
   "metadata": {},
   "source": [
    "The functions defining our full circuit, our weighted layers, our classifier, loss, accuracy and cost can all be defined as they were for the amplitude encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "35f97882",
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev)\n",
    "def circuit(weights, params):\n",
    "    state_preparation(params)\n",
    "\n",
    "    for W in weights:\n",
    "        layer(W)\n",
    "\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "def layer(W):\n",
    "    qml.Rot(W[0, 0], W[0, 1], W[0, 2], wires=0)\n",
    "    qml.Rot(W[1, 0], W[0, 1], W[1, 2], wires=1)\n",
    "    qml.CNOT(wires=[1, 0])\n",
    "\n",
    "def variational_classifier(weights, bias, params):\n",
    "    return circuit(weights, params) + bias\n",
    "\n",
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss += (l - p) ** 2\n",
    "    loss /= len(labels)\n",
    "    return loss\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l - p) < 1e-5:\n",
    "            loss = loss + 1\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "def cost(weights, bias, features, labels):\n",
    "    predictions = [variational_classifier(weights, bias, f) for f in features]\n",
    "    return square_loss(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16af33b8",
   "metadata": {},
   "source": [
    "The data is split into 80% training and 20% validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f064c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "num_data = len(Y)\n",
    "num_train = int(0.8 * num_data)\n",
    "index = np.random.permutation(range(num_data))\n",
    "feats_train = features[index[:num_train]]\n",
    "Y_train = Y[index[:num_train]]\n",
    "feats_val = features[index[num_train:]]\n",
    "Y_val = Y[index[num_train:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbb1ea8",
   "metadata": {},
   "source": [
    "We now need 4 qubits and choose to double the number of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "15833ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits = 4\n",
    "num_layers = 12\n",
    "\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "bias_init = np.array(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e35e094",
   "metadata": {},
   "source": [
    "The Adam optimizer is used and the circuit is trained over 60 steps. As we saw earlier, the cost is seen to steadily decease, and the accuracy of the training and validation sets increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "14de7263",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 1.4659866 | Acc train: 0.5291667 | Acc validation: 0.5666667 \n",
      "Iter:     2 | Cost: 1.4327932 | Acc train: 0.5250000 | Acc validation: 0.5500000 \n",
      "Iter:     3 | Cost: 1.4003558 | Acc train: 0.5375000 | Acc validation: 0.5333333 \n",
      "Iter:     4 | Cost: 1.3706410 | Acc train: 0.5416667 | Acc validation: 0.5500000 \n",
      "Iter:     5 | Cost: 1.3425421 | Acc train: 0.5375000 | Acc validation: 0.5500000 \n",
      "Iter:     6 | Cost: 1.3147193 | Acc train: 0.5375000 | Acc validation: 0.5333333 \n",
      "Iter:     7 | Cost: 1.2859176 | Acc train: 0.5458333 | Acc validation: 0.5166667 \n",
      "Iter:     8 | Cost: 1.2559611 | Acc train: 0.5583333 | Acc validation: 0.5166667 \n",
      "Iter:     9 | Cost: 1.2263932 | Acc train: 0.5625000 | Acc validation: 0.5166667 \n",
      "Iter:    10 | Cost: 1.1967621 | Acc train: 0.5666667 | Acc validation: 0.5166667 \n",
      "Iter:    11 | Cost: 1.1691493 | Acc train: 0.5833333 | Acc validation: 0.5166667 \n",
      "Iter:    12 | Cost: 1.1414791 | Acc train: 0.6000000 | Acc validation: 0.5500000 \n",
      "Iter:    13 | Cost: 1.1132417 | Acc train: 0.6166667 | Acc validation: 0.5833333 \n",
      "Iter:    14 | Cost: 1.0890142 | Acc train: 0.6166667 | Acc validation: 0.6000000 \n",
      "Iter:    15 | Cost: 1.0661313 | Acc train: 0.6208333 | Acc validation: 0.6333333 \n",
      "Iter:    16 | Cost: 1.0433832 | Acc train: 0.6208333 | Acc validation: 0.6333333 \n",
      "Iter:    17 | Cost: 1.0178352 | Acc train: 0.6291667 | Acc validation: 0.6500000 \n",
      "Iter:    18 | Cost: 0.9890701 | Acc train: 0.6375000 | Acc validation: 0.6500000 \n",
      "Iter:    19 | Cost: 0.9600965 | Acc train: 0.6375000 | Acc validation: 0.6500000 \n",
      "Iter:    20 | Cost: 0.9394813 | Acc train: 0.6541667 | Acc validation: 0.6500000 \n",
      "Iter:    21 | Cost: 0.9163073 | Acc train: 0.6583333 | Acc validation: 0.6666667 \n",
      "Iter:    22 | Cost: 0.8955012 | Acc train: 0.6708333 | Acc validation: 0.6666667 \n",
      "Iter:    23 | Cost: 0.8761895 | Acc train: 0.6833333 | Acc validation: 0.6666667 \n",
      "Iter:    24 | Cost: 0.8569928 | Acc train: 0.6791667 | Acc validation: 0.6500000 \n",
      "Iter:    25 | Cost: 0.8363503 | Acc train: 0.6958333 | Acc validation: 0.6500000 \n",
      "Iter:    26 | Cost: 0.8184152 | Acc train: 0.6875000 | Acc validation: 0.6666667 \n",
      "Iter:    27 | Cost: 0.8017380 | Acc train: 0.7000000 | Acc validation: 0.7000000 \n",
      "Iter:    28 | Cost: 0.7826713 | Acc train: 0.7041667 | Acc validation: 0.7000000 \n",
      "Iter:    29 | Cost: 0.7635997 | Acc train: 0.7083333 | Acc validation: 0.7166667 \n",
      "Iter:    30 | Cost: 0.7496732 | Acc train: 0.7416667 | Acc validation: 0.7166667 \n",
      "Iter:    31 | Cost: 0.7424906 | Acc train: 0.7416667 | Acc validation: 0.7500000 \n",
      "Iter:    32 | Cost: 0.7384591 | Acc train: 0.7583333 | Acc validation: 0.7333333 \n",
      "Iter:    33 | Cost: 0.7376114 | Acc train: 0.7666667 | Acc validation: 0.7500000 \n",
      "Iter:    34 | Cost: 0.7303708 | Acc train: 0.7791667 | Acc validation: 0.7333333 \n",
      "Iter:    35 | Cost: 0.7208635 | Acc train: 0.7833333 | Acc validation: 0.7333333 \n",
      "Iter:    36 | Cost: 0.7142601 | Acc train: 0.7916667 | Acc validation: 0.7500000 \n",
      "Iter:    37 | Cost: 0.7077506 | Acc train: 0.7791667 | Acc validation: 0.7666667 \n",
      "Iter:    38 | Cost: 0.7046964 | Acc train: 0.7833333 | Acc validation: 0.7666667 \n",
      "Iter:    39 | Cost: 0.7005759 | Acc train: 0.7916667 | Acc validation: 0.7500000 \n",
      "Iter:    40 | Cost: 0.6974546 | Acc train: 0.7916667 | Acc validation: 0.7500000 \n",
      "Iter:    41 | Cost: 0.6941848 | Acc train: 0.7875000 | Acc validation: 0.7666667 \n",
      "Iter:    42 | Cost: 0.6891152 | Acc train: 0.7958333 | Acc validation: 0.7666667 \n",
      "Iter:    43 | Cost: 0.6833156 | Acc train: 0.8041667 | Acc validation: 0.8000000 \n",
      "Iter:    44 | Cost: 0.6759891 | Acc train: 0.8000000 | Acc validation: 0.8166667 \n",
      "Iter:    45 | Cost: 0.6732124 | Acc train: 0.8041667 | Acc validation: 0.7666667 \n",
      "Iter:    46 | Cost: 0.6745586 | Acc train: 0.7875000 | Acc validation: 0.7833333 \n",
      "Iter:    47 | Cost: 0.6808120 | Acc train: 0.7833333 | Acc validation: 0.7833333 \n",
      "Iter:    48 | Cost: 0.6836662 | Acc train: 0.7750000 | Acc validation: 0.7833333 \n",
      "Iter:    49 | Cost: 0.6903275 | Acc train: 0.7666667 | Acc validation: 0.7833333 \n",
      "Iter:    50 | Cost: 0.6877563 | Acc train: 0.7666667 | Acc validation: 0.7833333 \n",
      "Iter:    51 | Cost: 0.6846520 | Acc train: 0.7666667 | Acc validation: 0.7833333 \n",
      "Iter:    52 | Cost: 0.6682079 | Acc train: 0.7791667 | Acc validation: 0.7833333 \n",
      "Iter:    53 | Cost: 0.6518423 | Acc train: 0.7875000 | Acc validation: 0.8000000 \n",
      "Iter:    54 | Cost: 0.6337531 | Acc train: 0.8083333 | Acc validation: 0.8000000 \n",
      "Iter:    55 | Cost: 0.6214358 | Acc train: 0.8333333 | Acc validation: 0.8000000 \n",
      "Iter:    56 | Cost: 0.6170570 | Acc train: 0.8416667 | Acc validation: 0.8000000 \n",
      "Iter:    57 | Cost: 0.6174386 | Acc train: 0.8500000 | Acc validation: 0.8166667 \n",
      "Iter:    58 | Cost: 0.6193287 | Acc train: 0.8541667 | Acc validation: 0.8166667 \n",
      "Iter:    59 | Cost: 0.6210622 | Acc train: 0.8583333 | Acc validation: 0.8500000 \n",
      "Iter:    60 | Cost: 0.6217447 | Acc train: 0.8250000 | Acc validation: 0.8500000 \n"
     ]
    }
   ],
   "source": [
    "from pennylane.optimize import AdamOptimizer\n",
    "\n",
    "opt = AdamOptimizer()\n",
    "batch_size = 10\n",
    "\n",
    "# train the variational classifier\n",
    "weights = weights_init\n",
    "bias = bias_init\n",
    "for it in range(60):\n",
    "    # Update the weights by one optimizer step\n",
    "    batch_index = np.random.randint(0, num_train, (batch_size,))\n",
    "    feats_train_batch = feats_train[batch_index]\n",
    "    Y_train_batch = Y_train[batch_index]\n",
    "    weights, bias, _, _ = opt.step(cost, weights, bias, feats_train_batch, Y_train_batch)\n",
    "\n",
    "    # Compute predictions on train and validation set\n",
    "    predictions_train = [np.sign(variational_classifier(weights, bias, f)) for f in feats_train]\n",
    "    predictions_val = [np.sign(variational_classifier(weights, bias, f)) for f in feats_val]\n",
    "\n",
    "    # Compute accuracy on train and validation set\n",
    "    acc_train = accuracy(Y_train, predictions_train)\n",
    "    acc_val = accuracy(Y_val, predictions_val)\n",
    "\n",
    "    print(\n",
    "        \"Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n",
    "        \"\".format(it + 1, cost(weights, bias, features, Y), acc_train, acc_val)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472dc9ae",
   "metadata": {},
   "source": [
    "We calculate the accuracy of our test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a6dad430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 features:\n",
      " [[1.91088 2.0944  2.0944  0.96593]\n",
      " [2.18279 0.      1.0472  1.     ]\n",
      " [2.48824 0.      0.      0.08716]]\n",
      "\n",
      "Testing accuracy: 0.9416666666666667\n"
     ]
    }
   ],
   "source": [
    "# process our test data like we did our train data\n",
    "\n",
    "test_features = test_set[:, 0:4]\n",
    "print(\"First 3 features:\\n\", test_features[0:3])\n",
    "\n",
    "Y_test = test_set[:, -1]\n",
    "\n",
    "predictions_test = [np.sign(variational_classifier(weights, bias, f)) for f in test_features]\n",
    "acc_test = accuracy(Y_test, predictions_test)\n",
    "print(\"\\nTesting accuracy: {}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5146095",
   "metadata": {},
   "source": [
    "The accuracy for the test set is found to be ~ 94%, demonstrating this model has good predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37050c8b",
   "metadata": {},
   "source": [
    "Noting that the third and fourth wires were not utilized with the former layout, I added two more rotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e48d9533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 1.4696014 | Acc train: 0.5291667 | Acc validation: 0.5666667 \n",
      "Iter:     2 | Cost: 1.4360756 | Acc train: 0.5250000 | Acc validation: 0.5500000 \n",
      "Iter:     3 | Cost: 1.4055779 | Acc train: 0.5250000 | Acc validation: 0.5333333 \n",
      "Iter:     4 | Cost: 1.3766791 | Acc train: 0.5375000 | Acc validation: 0.5500000 \n",
      "Iter:     5 | Cost: 1.3562691 | Acc train: 0.5375000 | Acc validation: 0.5500000 \n",
      "Iter:     6 | Cost: 1.3333253 | Acc train: 0.5458333 | Acc validation: 0.5333333 \n",
      "Iter:     7 | Cost: 1.3114533 | Acc train: 0.5375000 | Acc validation: 0.5333333 \n",
      "Iter:     8 | Cost: 1.2877446 | Acc train: 0.5416667 | Acc validation: 0.5333333 \n",
      "Iter:     9 | Cost: 1.2654644 | Acc train: 0.5416667 | Acc validation: 0.5166667 \n",
      "Iter:    10 | Cost: 1.2424248 | Acc train: 0.5500000 | Acc validation: 0.5166667 \n",
      "Iter:    11 | Cost: 1.2193529 | Acc train: 0.5500000 | Acc validation: 0.5166667 \n",
      "Iter:    12 | Cost: 1.1941335 | Acc train: 0.5541667 | Acc validation: 0.5166667 \n",
      "Iter:    13 | Cost: 1.1685734 | Acc train: 0.5625000 | Acc validation: 0.5166667 \n",
      "Iter:    14 | Cost: 1.1423210 | Acc train: 0.5625000 | Acc validation: 0.5166667 \n",
      "Iter:    15 | Cost: 1.1180081 | Acc train: 0.5875000 | Acc validation: 0.5166667 \n",
      "Iter:    16 | Cost: 1.0912462 | Acc train: 0.6000000 | Acc validation: 0.5166667 \n",
      "Iter:    17 | Cost: 1.0638883 | Acc train: 0.6041667 | Acc validation: 0.5333333 \n",
      "Iter:    18 | Cost: 1.0352178 | Acc train: 0.6125000 | Acc validation: 0.5833333 \n",
      "Iter:    19 | Cost: 1.0074513 | Acc train: 0.6291667 | Acc validation: 0.6000000 \n",
      "Iter:    20 | Cost: 0.9802437 | Acc train: 0.6291667 | Acc validation: 0.6000000 \n",
      "Iter:    21 | Cost: 0.9510078 | Acc train: 0.6541667 | Acc validation: 0.6166667 \n",
      "Iter:    22 | Cost: 0.9207293 | Acc train: 0.6541667 | Acc validation: 0.6333333 \n",
      "Iter:    23 | Cost: 0.8940464 | Acc train: 0.6666667 | Acc validation: 0.6500000 \n",
      "Iter:    24 | Cost: 0.8658155 | Acc train: 0.6625000 | Acc validation: 0.6666667 \n",
      "Iter:    25 | Cost: 0.8386425 | Acc train: 0.6583333 | Acc validation: 0.6666667 \n",
      "Iter:    26 | Cost: 0.8146627 | Acc train: 0.6583333 | Acc validation: 0.6666667 \n",
      "Iter:    27 | Cost: 0.7955966 | Acc train: 0.6666667 | Acc validation: 0.6666667 \n",
      "Iter:    28 | Cost: 0.7799671 | Acc train: 0.6750000 | Acc validation: 0.6833333 \n",
      "Iter:    29 | Cost: 0.7609721 | Acc train: 0.7041667 | Acc validation: 0.7000000 \n",
      "Iter:    30 | Cost: 0.7395449 | Acc train: 0.7458333 | Acc validation: 0.7166667 \n",
      "Iter:    31 | Cost: 0.7224844 | Acc train: 0.7916667 | Acc validation: 0.8000000 \n",
      "Iter:    32 | Cost: 0.7101872 | Acc train: 0.7958333 | Acc validation: 0.8000000 \n",
      "Iter:    33 | Cost: 0.7011496 | Acc train: 0.8333333 | Acc validation: 0.8333333 \n",
      "Iter:    34 | Cost: 0.6940694 | Acc train: 0.8500000 | Acc validation: 0.8166667 \n",
      "Iter:    35 | Cost: 0.6825546 | Acc train: 0.8500000 | Acc validation: 0.8166667 \n",
      "Iter:    36 | Cost: 0.6746244 | Acc train: 0.8500000 | Acc validation: 0.8000000 \n",
      "Iter:    37 | Cost: 0.6656935 | Acc train: 0.8416667 | Acc validation: 0.8166667 \n",
      "Iter:    38 | Cost: 0.6539909 | Acc train: 0.8291667 | Acc validation: 0.8000000 \n",
      "Iter:    39 | Cost: 0.6447314 | Acc train: 0.8125000 | Acc validation: 0.8000000 \n",
      "Iter:    40 | Cost: 0.6393214 | Acc train: 0.8125000 | Acc validation: 0.8000000 \n",
      "Iter:    41 | Cost: 0.6351222 | Acc train: 0.8125000 | Acc validation: 0.8000000 \n",
      "Iter:    42 | Cost: 0.6296827 | Acc train: 0.8125000 | Acc validation: 0.8000000 \n",
      "Iter:    43 | Cost: 0.6260354 | Acc train: 0.7875000 | Acc validation: 0.7833333 \n",
      "Iter:    44 | Cost: 0.6205182 | Acc train: 0.7875000 | Acc validation: 0.7833333 \n",
      "Iter:    45 | Cost: 0.6137699 | Acc train: 0.7916667 | Acc validation: 0.7833333 \n",
      "Iter:    46 | Cost: 0.6042733 | Acc train: 0.8125000 | Acc validation: 0.8000000 \n",
      "Iter:    47 | Cost: 0.5929514 | Acc train: 0.8125000 | Acc validation: 0.8000000 \n",
      "Iter:    48 | Cost: 0.5842735 | Acc train: 0.8125000 | Acc validation: 0.8000000 \n",
      "Iter:    49 | Cost: 0.5754226 | Acc train: 0.8125000 | Acc validation: 0.8000000 \n",
      "Iter:    50 | Cost: 0.5678219 | Acc train: 0.8125000 | Acc validation: 0.8000000 \n",
      "Iter:    51 | Cost: 0.5623270 | Acc train: 0.8291667 | Acc validation: 0.8500000 \n",
      "Iter:    52 | Cost: 0.5591022 | Acc train: 0.8291667 | Acc validation: 0.8500000 \n",
      "Iter:    53 | Cost: 0.5566068 | Acc train: 0.8333333 | Acc validation: 0.8166667 \n",
      "Iter:    54 | Cost: 0.5555149 | Acc train: 0.8416667 | Acc validation: 0.8500000 \n",
      "Iter:    55 | Cost: 0.5545042 | Acc train: 0.8500000 | Acc validation: 0.8500000 \n",
      "Iter:    56 | Cost: 0.5505006 | Acc train: 0.8500000 | Acc validation: 0.8500000 \n",
      "Iter:    57 | Cost: 0.5461573 | Acc train: 0.8416667 | Acc validation: 0.8333333 \n",
      "Iter:    58 | Cost: 0.5423281 | Acc train: 0.8333333 | Acc validation: 0.8166667 \n",
      "Iter:    59 | Cost: 0.5397398 | Acc train: 0.8291667 | Acc validation: 0.8333333 \n",
      "Iter:    60 | Cost: 0.5378792 | Acc train: 0.8291667 | Acc validation: 0.8500000 \n",
      "\n",
      "Testing accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "def layer(W):\n",
    "    qml.Rot(W[0, 0], W[0, 1], W[0, 2], wires=0)\n",
    "    qml.Rot(W[1, 0], W[1, 2], W[1, 0], wires=1)\n",
    "    qml.Rot(W[0, 1], W[0, 1], W[1, 2], wires=2)\n",
    "    qml.Rot(W[1, 0], W[0, 1], W[1, 0], wires=3)\n",
    "    qml.CNOT(wires=[1, 0])\n",
    "    \n",
    "num_qubits = 4\n",
    "num_layers = 12\n",
    "\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "bias_init = np.array(0.0, requires_grad=True)\n",
    "\n",
    "opt = AdamOptimizer()\n",
    "batch_size = 10\n",
    "\n",
    "# train the variational classifier\n",
    "weights = weights_init\n",
    "bias = bias_init\n",
    "for it in range(60):\n",
    "    # Update the weights by one optimizer step\n",
    "    batch_index = np.random.randint(0, num_train, (batch_size,))\n",
    "    feats_train_batch = feats_train[batch_index]\n",
    "    Y_train_batch = Y_train[batch_index]\n",
    "    weights, bias, _, _ = opt.step(cost, weights, bias, feats_train_batch, Y_train_batch)\n",
    "\n",
    "    # Compute predictions on train and validation set\n",
    "    predictions_train = [np.sign(variational_classifier(weights, bias, f)) for f in feats_train]\n",
    "    predictions_val = [np.sign(variational_classifier(weights, bias, f)) for f in feats_val]\n",
    "\n",
    "    # Compute accuracy on train and validation set\n",
    "    acc_train = accuracy(Y_train, predictions_train)\n",
    "    acc_val = accuracy(Y_val, predictions_val)\n",
    "\n",
    "    print(\n",
    "        \"Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n",
    "        \"\".format(it + 1, cost(weights, bias, features, Y), acc_train, acc_val)\n",
    "    )\n",
    "    \n",
    "\n",
    "predictions_test = [np.sign(variational_classifier(weights, bias, f)) for f in test_features]\n",
    "acc_test = accuracy(Y_test, predictions_test)\n",
    "print(\"\\nTesting accuracy: {}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f6fc52",
   "metadata": {},
   "source": [
    "This result was very similar to the last. In the next run, I will add more gates to the layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8de79f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:     1 | Cost: 1.4201673 | Acc train: 0.5166667 | Acc validation: 0.6000000 \n",
      "Iter:     2 | Cost: 1.3453872 | Acc train: 0.5291667 | Acc validation: 0.6000000 \n",
      "Iter:     3 | Cost: 1.2808992 | Acc train: 0.5250000 | Acc validation: 0.6166667 \n",
      "Iter:     4 | Cost: 1.2172721 | Acc train: 0.5333333 | Acc validation: 0.6333333 \n",
      "Iter:     5 | Cost: 1.1505229 | Acc train: 0.5458333 | Acc validation: 0.6666667 \n",
      "Iter:     6 | Cost: 1.0868215 | Acc train: 0.5666667 | Acc validation: 0.6833333 \n",
      "Iter:     7 | Cost: 1.0243271 | Acc train: 0.5708333 | Acc validation: 0.6666667 \n",
      "Iter:     8 | Cost: 0.9650377 | Acc train: 0.5875000 | Acc validation: 0.6833333 \n",
      "Iter:     9 | Cost: 0.9068989 | Acc train: 0.6041667 | Acc validation: 0.7166667 \n",
      "Iter:    10 | Cost: 0.8505675 | Acc train: 0.6375000 | Acc validation: 0.7166667 \n",
      "Iter:    11 | Cost: 0.8036229 | Acc train: 0.6666667 | Acc validation: 0.7333333 \n",
      "Iter:    12 | Cost: 0.7685566 | Acc train: 0.7166667 | Acc validation: 0.7500000 \n",
      "Iter:    13 | Cost: 0.7476410 | Acc train: 0.7375000 | Acc validation: 0.7500000 \n",
      "Iter:    14 | Cost: 0.7318882 | Acc train: 0.7625000 | Acc validation: 0.7500000 \n",
      "Iter:    15 | Cost: 0.7253140 | Acc train: 0.7958333 | Acc validation: 0.8166667 \n",
      "Iter:    16 | Cost: 0.7182588 | Acc train: 0.7875000 | Acc validation: 0.8166667 \n",
      "Iter:    17 | Cost: 0.7001246 | Acc train: 0.8041667 | Acc validation: 0.8500000 \n",
      "Iter:    18 | Cost: 0.6838642 | Acc train: 0.8250000 | Acc validation: 0.9000000 \n",
      "Iter:    19 | Cost: 0.6750061 | Acc train: 0.8208333 | Acc validation: 0.8666667 \n",
      "Iter:    20 | Cost: 0.6666962 | Acc train: 0.8166667 | Acc validation: 0.8666667 \n",
      "Iter:    21 | Cost: 0.6600520 | Acc train: 0.8041667 | Acc validation: 0.8500000 \n",
      "Iter:    22 | Cost: 0.6492334 | Acc train: 0.8083333 | Acc validation: 0.8333333 \n",
      "Iter:    23 | Cost: 0.6399847 | Acc train: 0.8041667 | Acc validation: 0.8333333 \n",
      "Iter:    24 | Cost: 0.6324278 | Acc train: 0.8125000 | Acc validation: 0.8333333 \n",
      "Iter:    25 | Cost: 0.6259065 | Acc train: 0.8166667 | Acc validation: 0.8333333 \n",
      "Iter:    26 | Cost: 0.6177818 | Acc train: 0.8208333 | Acc validation: 0.8166667 \n",
      "Iter:    27 | Cost: 0.6115706 | Acc train: 0.8250000 | Acc validation: 0.8000000 \n",
      "Iter:    28 | Cost: 0.6075065 | Acc train: 0.8208333 | Acc validation: 0.8000000 \n",
      "Iter:    29 | Cost: 0.5935089 | Acc train: 0.8208333 | Acc validation: 0.8166667 \n",
      "Iter:    30 | Cost: 0.5733056 | Acc train: 0.8333333 | Acc validation: 0.8166667 \n",
      "Iter:    31 | Cost: 0.5555929 | Acc train: 0.8416667 | Acc validation: 0.8166667 \n",
      "Iter:    32 | Cost: 0.5447152 | Acc train: 0.8500000 | Acc validation: 0.8333333 \n",
      "Iter:    33 | Cost: 0.5361243 | Acc train: 0.8541667 | Acc validation: 0.8333333 \n",
      "Iter:    34 | Cost: 0.5302234 | Acc train: 0.8541667 | Acc validation: 0.8500000 \n",
      "Iter:    35 | Cost: 0.5285694 | Acc train: 0.8625000 | Acc validation: 0.8500000 \n",
      "Iter:    36 | Cost: 0.5297353 | Acc train: 0.8500000 | Acc validation: 0.8333333 \n",
      "Iter:    37 | Cost: 0.5337239 | Acc train: 0.8458333 | Acc validation: 0.8166667 \n",
      "Iter:    38 | Cost: 0.5383781 | Acc train: 0.8416667 | Acc validation: 0.8166667 \n",
      "Iter:    39 | Cost: 0.5415841 | Acc train: 0.8333333 | Acc validation: 0.8500000 \n",
      "Iter:    40 | Cost: 0.5422274 | Acc train: 0.8333333 | Acc validation: 0.8333333 \n",
      "Iter:    41 | Cost: 0.5379315 | Acc train: 0.8333333 | Acc validation: 0.8333333 \n",
      "Iter:    42 | Cost: 0.5318665 | Acc train: 0.8291667 | Acc validation: 0.8500000 \n",
      "Iter:    43 | Cost: 0.5242914 | Acc train: 0.8375000 | Acc validation: 0.8166667 \n",
      "Iter:    44 | Cost: 0.5194343 | Acc train: 0.8375000 | Acc validation: 0.8333333 \n",
      "Iter:    45 | Cost: 0.5115876 | Acc train: 0.8458333 | Acc validation: 0.8333333 \n",
      "Iter:    46 | Cost: 0.5056175 | Acc train: 0.8541667 | Acc validation: 0.8666667 \n",
      "Iter:    47 | Cost: 0.5049844 | Acc train: 0.8583333 | Acc validation: 0.8666667 \n",
      "Iter:    48 | Cost: 0.5072310 | Acc train: 0.8583333 | Acc validation: 0.8666667 \n",
      "Iter:    49 | Cost: 0.5108646 | Acc train: 0.8666667 | Acc validation: 0.8666667 \n",
      "Iter:    50 | Cost: 0.5200791 | Acc train: 0.8583333 | Acc validation: 0.8500000 \n",
      "Iter:    51 | Cost: 0.5191252 | Acc train: 0.8583333 | Acc validation: 0.8500000 \n",
      "Iter:    52 | Cost: 0.5196111 | Acc train: 0.8541667 | Acc validation: 0.8500000 \n",
      "Iter:    53 | Cost: 0.5174062 | Acc train: 0.8458333 | Acc validation: 0.8333333 \n",
      "Iter:    54 | Cost: 0.5125732 | Acc train: 0.8500000 | Acc validation: 0.8500000 \n",
      "Iter:    55 | Cost: 0.5082822 | Acc train: 0.8541667 | Acc validation: 0.8500000 \n",
      "Iter:    56 | Cost: 0.5024340 | Acc train: 0.8666667 | Acc validation: 0.8500000 \n",
      "Iter:    57 | Cost: 0.4941924 | Acc train: 0.8625000 | Acc validation: 0.8666667 \n",
      "Iter:    58 | Cost: 0.4877281 | Acc train: 0.8666667 | Acc validation: 0.8666667 \n",
      "Iter:    59 | Cost: 0.4841310 | Acc train: 0.8666667 | Acc validation: 0.8666667 \n",
      "Iter:    60 | Cost: 0.4818336 | Acc train: 0.8708333 | Acc validation: 0.8666667 \n",
      "\n",
      "Testing accuracy: 0.9833333333333333\n"
     ]
    }
   ],
   "source": [
    "def layer(W):\n",
    "    qml.Rot(W[0, 0], W[0, 1], W[0, 2], wires=0)\n",
    "    qml.Rot(W[1, 0], W[1, 2], W[1, 0], wires=1)\n",
    "    qml.Rot(W[0, 1], W[0, 1], W[1, 2], wires=2)\n",
    "    qml.Rot(W[1, 0], W[0, 1], W[1, 0], wires=3)\n",
    "    qml.CNOT(wires=[1, 0])\n",
    "    qml.CNOT(wires=[3, 2])\n",
    "    qml.Rot(W[0, 1], W[1, 1], W[1, 2], wires=0)\n",
    "    qml.Rot(W[1, 0], W[2, 2], W[1, 2], wires=1)\n",
    "    qml.Rot(W[1, 1], W[0, 0], W[0, 2], wires=2)\n",
    "    qml.Rot(W[1, 2], W[1, 0], W[2, 0], wires=3)\n",
    "    qml.CNOT(wires=[3, 1])\n",
    "    qml.CNOT(wires=[2, 0])\n",
    "    \n",
    "num_qubits = 4\n",
    "num_layers = 12\n",
    "\n",
    "weights_init = 0.01 * np.random.randn(num_layers, num_qubits, 3, requires_grad=True)\n",
    "bias_init = np.array(0.0, requires_grad=True)\n",
    "\n",
    "opt = AdamOptimizer()\n",
    "batch_size = 10\n",
    "\n",
    "# train the variational classifier\n",
    "weights = weights_init\n",
    "bias = bias_init\n",
    "for it in range(60):\n",
    "    # Update the weights by one optimizer step\n",
    "    batch_index = np.random.randint(0, num_train, (batch_size,))\n",
    "    feats_train_batch = feats_train[batch_index]\n",
    "    Y_train_batch = Y_train[batch_index]\n",
    "    weights, bias, _, _ = opt.step(cost, weights, bias, feats_train_batch, Y_train_batch)\n",
    "\n",
    "    # Compute predictions on train and validation set\n",
    "    predictions_train = [np.sign(variational_classifier(weights, bias, f)) for f in feats_train]\n",
    "    predictions_val = [np.sign(variational_classifier(weights, bias, f)) for f in feats_val]\n",
    "\n",
    "    # Compute accuracy on train and validation set\n",
    "    acc_train = accuracy(Y_train, predictions_train)\n",
    "    acc_val = accuracy(Y_val, predictions_val)\n",
    "\n",
    "    print(\n",
    "        \"Iter: {:5d} | Cost: {:0.7f} | Acc train: {:0.7f} | Acc validation: {:0.7f} \"\n",
    "        \"\".format(it + 1, cost(weights, bias, features, Y), acc_train, acc_val)\n",
    "    )\n",
    "    \n",
    "\n",
    "predictions_test = [np.sign(variational_classifier(weights, bias, f)) for f in test_features]\n",
    "acc_test = accuracy(Y_test, predictions_test)\n",
    "print(\"\\nTesting accuracy: {}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db294a6d",
   "metadata": {},
   "source": [
    "The cost was lower than in the last run, and the accuracy on both the training and validation sets was higher. The accuracy on the test set was higher as well, at about 98%. The angle encoding appeared to benefit from this deeper circuit. One downside of this deeper circuit is that training took a longer amount of time to finish."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
